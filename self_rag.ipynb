{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evaluate_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\"\n",
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if retrieval is necessary\", description=\"Output only 'Yes' or 'No'.\")\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Given the query '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
    ")\n",
    "\n",
    "class RelevanceResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if context is relevant\", description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
    ")\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Generated response\", description=\"The generated response.\")\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
    ")\n",
    "\n",
    "class SupportResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if response is supported\", description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
    "support_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\", \"context\"],\n",
    "    template=\"Given the response '{response}' and the context '{context}', determine if the response is supported by the context. Output 'Fully supported', 'Partially supported', or 'No support'.\"\n",
    ")\n",
    "\n",
    "class UtilityResponse(BaseModel):\n",
    "    response: int = Field(..., title=\"Utility rating\", description=\"Rate the utility of the response from 1 to 5.\")\n",
    "utility_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"response\"],\n",
    "    template=\"Given the query '{query}' and the response '{response}', rate the utility of the response from 1 to 5.\"\n",
    ")\n",
    "\n",
    "# Create LLMChains for each step\n",
    "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
    "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
    "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
    "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
    "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vectorstore, top_k=3):\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    # Step 1: Determine if retrieval is necessary\n",
    "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "    input_data = {\"query\": query}\n",
    "    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n",
    "    print(f\"Retrieval decision: {retrieval_decision}\")\n",
    "    \n",
    "    if retrieval_decision == 'yes':\n",
    "        # Step 2: Retrieve relevant documents\n",
    "        print(\"Step 2: Retrieving relevant documents...\")\n",
    "        docs = vectorstore.similarity_search(query, k=top_k)\n",
    "        contexts = [doc.page_content for doc in docs]\n",
    "        print(f\"Retrieved {len(contexts)} documents\")\n",
    "        \n",
    "        # Step 3: Evaluate relevance of retrieved documents\n",
    "        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
    "        relevant_contexts = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n",
    "            print(f\"Document {i+1} relevance: {relevance}\")\n",
    "            if relevance == 'relevant':\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
    "        \n",
    "        # If no relevant contexts found, generate without retrieval\n",
    "        if not relevant_contexts:\n",
    "            print(\"No relevant contexts found. Generating without retrieval...\")\n",
    "            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
    "            return generation_chain.invoke(input_data).response\n",
    "        \n",
    "        # Step 4: Generate response using relevant contexts\n",
    "        print(\"Step 4: Generating responses using relevant contexts...\")\n",
    "        responses = []\n",
    "        for i, context in enumerate(relevant_contexts):\n",
    "            print(f\"Generating response for context {i+1}...\")\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            response = generation_chain.invoke(input_data).response\n",
    "            \n",
    "            # Step 5: Assess support\n",
    "            print(f\"Step 5: Assessing support for response {i+1}...\")\n",
    "            input_data = {\"response\": response, \"context\": context}\n",
    "            support = support_chain.invoke(input_data).response.strip().lower()\n",
    "            print(f\"Support assessment: {support}\")\n",
    "            \n",
    "            # Step 6: Evaluate utility\n",
    "            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n",
    "            input_data = {\"query\": query, \"response\": response}\n",
    "            utility = int(utility_chain.invoke(input_data).response)\n",
    "            print(f\"Utility score: {utility}\")\n",
    "            \n",
    "            responses.append((response, support, utility))\n",
    "        \n",
    "        # Select the best response based on support and utility\n",
    "        print(\"Selecting the best response...\")\n",
    "        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
    "        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
    "        return best_response[0]\n",
    "    else:\n",
    "        # Generate without retrieval\n",
    "        print(\"Generating without retrieval...\")\n",
    "        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n",
    "        return generation_chain.invoke(input_data).response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
