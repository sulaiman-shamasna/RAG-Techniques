{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import necessary Libraries and Packages\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from enum import Enum\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate a Question Generation Class and Define some Parameters\n",
    "\"\"\"\n",
    "\n",
    "class QuestionGeneration(Enum):\n",
    "    \"\"\"\n",
    "    Enum class to specify the level of question generation for document processing.\n",
    "\n",
    "    Attributes:\n",
    "        DOCUMENT_LEVEL (int): Represents question generation at the entire document level.\n",
    "        FRAGMENT_LEVEL (int): Represents question generation at the individual text fragment level.\n",
    "    \"\"\"\n",
    "    DOCUMENT_LEVEL = 1\n",
    "    FRAGMENT_LEVEL = 2\n",
    "\n",
    "#Depending on the model, for Mitral 7B it can be max 8000, for Llama 3.1 8B 128k\n",
    "DOCUMENT_MAX_TOKENS = 4000\n",
    "DOCUMENT_OVERLAP_TOKENS = 100\n",
    "\n",
    "#Embeddings and text similarity calculated on shorter texts\n",
    "FRAGMENT_MAX_TOKENS = 128\n",
    "FRAGMENT_OVERLAP_TOKENS = 16\n",
    "\n",
    "#Questions generated on document or fragment level\n",
    "QUESTION_GENERATION = QuestionGeneration.DOCUMENT_LEVEL\n",
    "#how many questions will be generated for specific document or fragment\n",
    "QUESTIONS_PER_DOCUMENT = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define classes and functions used by this pipeline\n",
    "\"\"\"\n",
    "\n",
    "class QuestionList(BaseModel):\n",
    "    question_list: List[str] = Field(..., title=\"List of questions generated for the document or fragment\")\n",
    "\n",
    "\n",
    "class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):\n",
    "    \"\"\"\n",
    "    A wrapper class for OpenAI embeddings, providing a similar interface to the original OllamaEmbeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Allows the instance to be used as a callable to generate an embedding for a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The embedding for the query as a list of floats.\n",
    "        \"\"\"\n",
    "        return self.embed_query(query)\n",
    "\n",
    "def clean_and_filter_questions(questions: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans and filters a list of questions.\n",
    "\n",
    "    Args:\n",
    "        questions (List[str]): A list of questions to be cleaned and filtered.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of cleaned and filtered questions that end with a question mark.\n",
    "    \"\"\"\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('?'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "def generate_questions(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions based on the provided text using OpenAI.\n",
    "\n",
    "    Args:\n",
    "        text (str): The context data from which questions are generated.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique, filtered questions.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": QUESTIONS_PER_DOCUMENT}\n",
    "    result = chain.invoke(input_data)\n",
    "    \n",
    "    # Extract the list of questions from the QuestionList object\n",
    "    questions = result.question_list\n",
    "    \n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "def generate_answer(content: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to a given question based on the provided context using OpenAI.\n",
    "\n",
    "    Args:\n",
    "        content (str): The context data used to generate the answer.\n",
    "        question (str): The question for which the answer is generated.\n",
    "\n",
    "    Returns:\n",
    "        str: The precise answer to the question based on the provided context.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"Using the context data: {context}\\n\\nProvide a brief and precise answer to the question: {question}\"\n",
    "    )\n",
    "    chain =  prompt | llm\n",
    "    input_data = {\"context\": content, \"question\": question}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a document into smaller chunks of text.\n",
    "\n",
    "    Args:\n",
    "        document (str): The text of the document to be split.\n",
    "        chunk_size (int): The size of each chunk in terms of the number of tokens.\n",
    "        chunk_overlap (int): The number of overlapping tokens between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks, where each chunk is a string of the document content.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "def print_document(comment: str, document: Any) -> None:\n",
    "    \"\"\"\n",
    "    Prints a comment followed by the content of a document.\n",
    "\n",
    "    Args:\n",
    "        comment (str): The comment or description to print before the document details.\n",
    "        document (Any): The document whose content is to be printed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f'{comment} (type: {document.metadata[\"type\"]}, index: {document.metadata[\"index\"]}): {document.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "- Is the document suitable for beginners?\n",
      "- Are there any illustrations or diagrams in the document?\n",
      "- What is the length of the document?\n",
      "- Does the document provide detailed explanations of topics?\n",
      "- Are there any limitations mentioned in the document?\n",
      "- What type of information does the document contain?\n",
      "- Can the document be used as a reference?\n",
      "- Is the document written in a formal or informal tone?\n",
      "- Are there any key terms defined in the document?\n",
      "- Are there any specific topics mentioned in the document?\n",
      "- Are there any testimonials included in the document?\n",
      "- What is the purpose of the document?\n",
      "- Are there any visual aids included in the document?\n",
      "- Are there any historical references in the document?\n",
      "- Is the document part of a series or collection?\n",
      "- Does the document provide contact information for further inquiries?\n",
      "- Are there any complex topics covered in the document?\n",
      "- Is the document structured in a particular way?\n",
      "- Does the document include examples of the topics discussed?\n",
      "- How many topics are covered in the document?\n",
      "- Does the document address any controversies related to the topics?\n",
      "- Are there any notable authors or contributors to the document?\n",
      "- Are there any case studies mentioned in the document?\n",
      "- Are there any recommendations provided in the document?\n",
      "- Is the document intended for a particular audience?\n",
      "- What format is the document presented in?\n",
      "- Does the document provide a summary of the topics?\n",
      "- Does the document include citations or references?\n",
      "- Is the document intended for professionals in a specific field?\n",
      "- Does the document provide insights or opinions on the topics?\n",
      "- Does the document discuss current trends related to the topics?\n",
      "- Does the document contain any statistics or data?\n",
      "- Is the document organized by topic or theme?\n",
      "- Does the document have a publication date?\n",
      "- Does the document have a table of contents?\n",
      "- Is the information in the document up to date?\n",
      "- Is the document available in multiple languages?\n",
      "- Can the document be used for educational purposes?\n",
      "- Is the document interactive in any way?\n",
      "- Is the document accessible online?\n",
      "- Does the document include any appendices?\n",
      "- Is the document peer-reviewed?\n",
      "- Does the document include a glossary of terms?\n",
      "- Are there any follow-up resources suggested in the document?\n",
      "- Is the document free to access?\n",
      "- Is the document an example or a guide?\n",
      "\n",
      "Question: Is the document suitable for beginners?\n",
      "Answer: content='Yes, the document is suitable for beginners as it contains information about various topics.' response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 42, 'total_tokens': 58}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None} id='run-1ead5027-ef40-482d-9658-24a4eddf595f-0' usage_metadata={'input_tokens': 42, 'output_tokens': 16, 'total_tokens': 58}\n",
      "\n",
      "Document Chunks:\n",
      "Chunk 1: This is an example document It contains information about various\n",
      "Chunk 2: about various topics\n",
      "\n",
      "Document Embedding (first 5 elements): [-0.009086555801331997, 0.023263173177838326, 0.0058456179685890675, -0.0183719415217638, -0.009146205149590969]\n",
      "Query Embedding (first 5 elements): [0.008873074315488338, 0.002703973324969411, 0.0010121563682332635, -0.0037226493004709482, -0.018711037933826447]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Usage\n",
    "\"\"\"\n",
    "\n",
    "# Initialize OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddingsWrapper()\n",
    "\n",
    "# Example document\n",
    "example_text = \"This is an example document. It contains information about various topics.\"\n",
    "\n",
    "# Generate questions\n",
    "questions = generate_questions(example_text)\n",
    "print(\"Generated Questions:\")\n",
    "for q in questions:\n",
    "    print(f\"- {q}\")\n",
    "\n",
    "# Generate an answer\n",
    "sample_question = questions[0] if questions else \"What is this document about?\"\n",
    "answer = generate_answer(example_text, sample_question)\n",
    "print(f\"\\nQuestion: {sample_question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "# Split document\n",
    "chunks = split_document(example_text, chunk_size=10, chunk_overlap=2)\n",
    "print(\"\\nDocument Chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")\n",
    "\n",
    "# Example of using OpenAIEmbeddings\n",
    "doc_embedding = embeddings.embed_documents([example_text])\n",
    "query_embedding = embeddings.embed_query(\"What is the main topic?\")\n",
    "print(\"\\nDocument Embedding (first 5 elements):\", doc_embedding[0][:5])\n",
    "print(\"Query Embedding (first 5 elements):\", query_embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
