{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import necessary Libraries and Packages\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from enum import Enum\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate a Question Generation Class and Define some Parameters\n",
    "\"\"\"\n",
    "\n",
    "class QuestionGeneration(Enum):\n",
    "    \"\"\"\n",
    "    Enum class to specify the level of question generation for document processing.\n",
    "\n",
    "    Attributes:\n",
    "        DOCUMENT_LEVEL (int): Represents question generation at the entire document level.\n",
    "        FRAGMENT_LEVEL (int): Represents question generation at the individual text fragment level.\n",
    "    \"\"\"\n",
    "    DOCUMENT_LEVEL = 1\n",
    "    FRAGMENT_LEVEL = 2\n",
    "\n",
    "#Depending on the model, for Mitral 7B it can be max 8000, for Llama 3.1 8B 128k\n",
    "DOCUMENT_MAX_TOKENS = 4000\n",
    "DOCUMENT_OVERLAP_TOKENS = 100\n",
    "\n",
    "#Embeddings and text similarity calculated on shorter texts\n",
    "FRAGMENT_MAX_TOKENS = 128\n",
    "FRAGMENT_OVERLAP_TOKENS = 16\n",
    "\n",
    "#Questions generated on document or fragment level\n",
    "QUESTION_GENERATION = QuestionGeneration.DOCUMENT_LEVEL\n",
    "#how many questions will be generated for specific document or fragment\n",
    "QUESTIONS_PER_DOCUMENT = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define classes and functions used by this pipeline\n",
    "\"\"\"\n",
    "\n",
    "class QuestionList(BaseModel):\n",
    "    question_list: List[str] = Field(..., title=\"List of questions generated for the document or fragment\")\n",
    "\n",
    "\n",
    "class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):\n",
    "    \"\"\"\n",
    "    A wrapper class for OpenAI embeddings, providing a similar interface to the original OllamaEmbeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Allows the instance to be used as a callable to generate an embedding for a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The embedding for the query as a list of floats.\n",
    "        \"\"\"\n",
    "        return self.embed_query(query)\n",
    "\n",
    "def clean_and_filter_questions(questions: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans and filters a list of questions.\n",
    "\n",
    "    Args:\n",
    "        questions (List[str]): A list of questions to be cleaned and filtered.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of cleaned and filtered questions that end with a question mark.\n",
    "    \"\"\"\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('?'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "def generate_questions(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions based on the provided text using OpenAI.\n",
    "\n",
    "    Args:\n",
    "        text (str): The context data from which questions are generated.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique, filtered questions.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": QUESTIONS_PER_DOCUMENT}\n",
    "    result = chain.invoke(input_data)\n",
    "    \n",
    "    # Extract the list of questions from the QuestionList object\n",
    "    questions = result.question_list\n",
    "    \n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "def generate_answer(content: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to a given question based on the provided context using OpenAI.\n",
    "\n",
    "    Args:\n",
    "        content (str): The context data used to generate the answer.\n",
    "        question (str): The question for which the answer is generated.\n",
    "\n",
    "    Returns:\n",
    "        str: The precise answer to the question based on the provided context.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"Using the context data: {context}\\n\\nProvide a brief and precise answer to the question: {question}\"\n",
    "    )\n",
    "    chain =  prompt | llm\n",
    "    input_data = {\"context\": content, \"question\": question}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a document into smaller chunks of text.\n",
    "\n",
    "    Args:\n",
    "        document (str): The text of the document to be split.\n",
    "        chunk_size (int): The size of each chunk in terms of the number of tokens.\n",
    "        chunk_overlap (int): The number of overlapping tokens between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks, where each chunk is a string of the document content.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "def print_document(comment: str, document: Any) -> None:\n",
    "    \"\"\"\n",
    "    Prints a comment followed by the content of a document.\n",
    "\n",
    "    Args:\n",
    "        comment (str): The comment or description to print before the document details.\n",
    "        document (Any): The document whose content is to be printed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f'{comment} (type: {document.metadata[\"type\"]}, index: {document.metadata[\"index\"]}): {document.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
